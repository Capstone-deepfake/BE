# backend/deepfake/views.py

import os
import cv2
import uuid
import traceback
import base64
import gc

from django.conf import settings
from django.http import JsonResponse
from django.views.decorators.csrf import csrf_exempt

import torch
from torchvision import transforms
from PIL import Image

from .modelNet import DeepfakeDetector
import face_recognition
from .gradcam import apply_gradcam_to_image

from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()
client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])


def get_gpt_explanation_with_data_uri(image_path: str) -> str:
    """
    ë¡œì»¬ì— ì €ìž¥ëœ Grad-CAM ì´ë¯¸ì§€ë¥¼ Base64 â†’ Data URIë¡œ ê°ì‹¸
    'image_url' ë©”ì‹œì§€ ë¸”ë¡ì— ë„£ì–´ OpenAIë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    try:
        with open(image_path, "rb") as f:
            img_b64 = base64.b64encode(f.read()).decode("utf-8")
        data_uri = f"data:image/jpeg;base64,{img_b64}"

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": (
                                "í•´ë‹¹ ì‚¬ì§„ì´ ë”¥íŽ˜ì´í¬ì¸ ê·¼ê±°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ëª…í•´ì¤˜: "
                                "ì–¼êµ´ê³¼ ë°°ê²½ ì‚¬ì´ ê²½ê³„ì„ ì´ ë¶€ìžì—°ìŠ¤ëŸ½ê²Œ ì¼ê·¸ëŸ¬ì§€ê³ , "
                                "ê¹œë¹¡ì´ëŠ” í˜„ìƒì´ ë°œìƒí•©ë‹ˆë‹¤. ë˜ ì–¼êµ´ í‘œì •ì´ ë§ê³¼ ì–´ìš¸ë¦¬ì§€ ì•Šìœ¼ë©° "
                                "ëˆˆ ê¹œë¹¡ìž„ ë¹ˆë„ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ì§€ë‚˜ì¹©ë‹ˆë‹¤. ì–¼êµ´ ì´ì™¸ë¡œ ì¡°ëª…ê³¼ "
                                "ì£¼ë³€ í™˜ê²½ìœ¼ë¡œ ê·¸ë¦¼ìžì˜ ë°©í–¥ì´ ì´ìƒí•˜ê³  ë§ í•˜ëŠ” ë‚´ìš©ê³¼ ìž…ìˆ  ì›€ì§ìž„ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                            )
                        },
                        {
                            "type": "image_url",
                            "image_url": {"url": data_uri}
                        }
                    ]
                }
            ],
            max_tokens=300
        )
        return response.choices[0].message.content

    except Exception as e:
        print("OpenAI API error:", e)
        return "ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# â”€â”€â”€ ëª¨ë¸ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

model = DeepfakeDetector()
checkpoint_path = os.path.join(settings.BASE_DIR, "model", "checkpoint_1.pt")
if not os.path.exists(checkpoint_path):
    raise FileNotFoundError(f"ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
state_dict = torch.load(checkpoint_path, map_location=device)
model.load_state_dict(state_dict, strict=False)
model.to(device)
model.eval()

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])


def extract_frames(video_path):
    """
    ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ í”„ë ˆìž„ì„ í•œ ìž¥ì”© ë°˜í™˜í•˜ëŠ” ì œë„ˆë ˆì´í„°.
    finallyì ˆì—ì„œ cap.release() í˜¸ì¶œ.
    """
    cap = cv2.VideoCapture(video_path)
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            yield frame
    finally:
        cap.release()


# â”€â”€â”€ íŒŒì¼ ì—…ë¡œë“œ ë° ê²€ì¶œ ë·° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@csrf_exempt
def upload_file(request):
    try:
        if request.method != "POST" or "file" not in request.FILES:
            return JsonResponse({"error": "ìž˜ëª»ëœ ìš”ì²­"}, status=400)

        # 1) ì—…ë¡œë“œëœ ë¹„ë””ì˜¤ ì €ìž¥
        uploaded_file = request.FILES["file"]
        upload_id = str(uuid.uuid4())
        video_dir = os.path.join(settings.MEDIA_ROOT, "uploads", upload_id)
        os.makedirs(video_dir, exist_ok=True)

        video_path = os.path.join(video_dir, uploaded_file.name)
        with open(video_path, "wb") as f:
            for chunk in uploaded_file.chunks():
                f.write(chunk)
        print(f"[DEBUG] video saved: {video_path}")

        # 2) ì–¼êµ´ í¬ë¡­ í”„ë ˆìž„ ì¶”ì¶œ
        frames_dir = os.path.join(video_dir, "frames")
        os.makedirs(frames_dir, exist_ok=True)

        saved_index = 0
        for idx, frame in enumerate(extract_frames(video_path)):
            if frame is None:
                break

            rgb = frame[:, :, ::-1]  # BGR â†’ RGB
            faces = face_recognition.face_locations(rgb)
            print(f"[DEBUG] frame#{idx}: faces found = {len(faces)}")

            if faces:
                top, right, bottom, left = faces[0]
                face_img = rgb[top:bottom, left:right]
                h, w, _ = face_img.shape
                if h >= 32 and w >= 32:
                    face_bgr = cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR)
                    fname = f"frame_{saved_index:04d}.jpg"
                    out_path = os.path.join(frames_dir, fname)
                    cv2.imwrite(out_path, face_bgr)
                    saved_index += 1
                    print(f"[DEBUG]   saved cropped face: {fname} ({w}Ã—{h})")

            if saved_index >= 150:
                break

        print(f"[DEBUG] total cropped frames saved: {saved_index}")
        frame_files = sorted(os.listdir(frames_dir))
        print(f"[DEBUG] frame_files ({len(frame_files)}): {frame_files[:5]}")

        if not frame_files:
            print("[ERROR] Frame extraction failed: no cropped frames.")
            return JsonResponse(
                {"error": "í”„ë ˆìž„ ì¶”ì¶œ ì‹¤íŒ¨ (ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤)"}, status=500
            )

        # 3) ëª¨ë¸ ì¶”ë¡  (í”„ë ˆìž„ë³„ fake í™•ë¥  ìˆ˜ì§‘)
        predictions = []
        score_list  = []
        for fname in frame_files:
            img_path = os.path.join(frames_dir, fname)
            image = Image.open(img_path).convert("RGB")
            input_tensor = preprocess(image).unsqueeze(0).to(device)
            with torch.no_grad():
                outputs = model(input_tensor)
                probs = torch.softmax(outputs, dim=1)[0].tolist()
                pred  = torch.argmax(outputs, dim=1).item()

            predictions.append(pred)
            score_list.append(probs[1])
            print(f"[DEBUG] frame={fname} â†’ P(fake)={probs[1]:.3f}, pred={pred}")

        # 4) ì¢…ë£Œ í›„ ë¶„ë¥˜: fake_ratioë§Œ 0.5 ê¸°ì¤€
        fake_count   = sum(predictions)
        total_frames = len(predictions)
        fake_ratio   = fake_count / total_frames
        print(f"[DEBUG] fake_ratio={fake_ratio:.3f}, frames={total_frames}")

        if fake_ratio >= 0.5:
            result_label = "Fake"
            confidence   = round(fake_ratio * 100, 1)
        else:
            result_label = "Real"
            confidence   = round((1 - fake_ratio) * 100, 1)

        file_url = settings.MEDIA_URL + f"uploads/{upload_id}/{uploaded_file.name}"

        # 5) Realì´ë©´ ë°”ë¡œ ë°˜í™˜
        if result_label == "Real":
            print(f"[DEBUG] Final result: REAL (confidence {confidence}%)")
            return JsonResponse({
                "file_url":    file_url,
                "result":      result_label,
                "confidence":  confidence,
                "explanation": "ì´ ì˜ìƒì€ Realë¡œ íŒë³„ë˜ì—ˆìŠµë‹ˆë‹¤.",
            })

        # 6) Fakeì¼ ë•Œ Grad-CAM + ì„¤ëª… ìš”ì²­
        target_frame = os.path.join(frames_dir, frame_files[0])
        print(f"[DEBUG] Using {frame_files[0]} for Grad-CAM")
        gradcam_dir  = os.path.join(video_dir, "gradcam")
        os.makedirs(gradcam_dir, exist_ok=True)
        gradcam_path = os.path.join(gradcam_dir, "gradcam_result.jpg")

        apply_gradcam_to_image(model, target_frame, model.conv1, gradcam_path)
        print(f"[DEBUG] Grad-CAM saved: {gradcam_path}")

        # Matplotlib figure ë‹«ê¸° (gradcam ë‚´ë¶€ì—ì„œ plt ì‚¬ìš© ì‹œ)
        try:
            import matplotlib.pyplot as plt
            plt.close("all")
        except:
            pass

        explanation = get_gpt_explanation_with_data_uri(gradcam_path)
        print(f"[DEBUG] OpenAI explanation: {explanation[:60]}...")

        # 7) ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        torch.cuda.empty_cache()
        gc.collect()

        return JsonResponse({
            "file_url":    file_url,
            "result":      result_label,
            "confidence":  confidence,
            "gradcam_url": None,
            "explanation": explanation,
        })

    except Exception as e:
        print("[ERROR]", traceback.format_exc())
        # crash ëŒ€ì‹  JSON ì˜¤ë¥˜ ì‘ë‹µ
        return JsonResponse({"error": str(e)}, status=500)


# â”€â”€â”€ ì±„íŒ… ë·° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@csrf_exempt
def chat_with_ai(request):
    try:
        if request.method != "POST":
            return JsonResponse({"error": "Only POST method is allowed"}, status=405)

        import json
        data = json.loads(request.body)
        user_message = data.get("message")
        video_context = data.get("context", {})

        if not user_message:
            return JsonResponse({"error": "Message is required"}, status=400)

        system_message = """You are DE-fake it's AI expert assistant specialized in deepfake detection. 
        You are knowledgeable about various deepfake detection techniques, their implications, and how to interpret results.
        Always maintain a friendly and professional tone, using emojis appropriately in your responses.
        When discussing confidence scores, explain what they mean in practical terms.
        Help users understand the implications of the detection results and what they should do next."""

        context_message = f"""Analysis Context ðŸ”:
        - Detection Result: {video_context.get('result','Unknown')} {'ðŸš«' if video_context.get('result')=='Fake' else 'âœ…'}
        - Confidence Score: {video_context.get('confidence',0)}% {'ðŸŽ¯' if video_context.get('confidence',0)>80 else 'ðŸ“Š'}
        
        User Question: {user_message}"""

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user",   "content": context_message},
            ],
            max_tokens=500,
            temperature=0.7,
        )

        return JsonResponse({"response": response.choices[0].message.content})

    except Exception as e:
        print("[ERROR]", traceback.format_exc())
        return JsonResponse({"error": str(e)}, status=500)
