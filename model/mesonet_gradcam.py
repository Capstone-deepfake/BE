# -*- coding: utf-8 -*-
"""Untitled32.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c7dB-vbi7rLui3U9yN4aVcgaW5PyVa7H
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
import matplotlib.pyplot as plt
from torchvision import transforms
import matplotlib.cm as cm
import pandas as pd
import random
from pathlib import Path

# Configuration matching the trained model
TRAINED_CONFIG = {
    'batch_size': 32,
    'learning_rate': 0.0001,
    'frame_count': 32,  # Updated from 20 to 32 to match trained model
    'image_size': 128,
    'lstm_hidden_size': 256,  # Updated from 128 to 256
    'lstm_layers': 2,
    'dropout_rate': 0.3,  # Updated from 0.5 to 0.3
    'weight_decay': 0.0001,
    'gradient_clip': 1.0,
    'name': 'base_config'
}

# Use trained config values
FRAME_COUNT = TRAINED_CONFIG['frame_count']
IMAGE_SIZE = TRAINED_CONFIG['image_size']
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ------------------------------
# Enhanced Model Architecture (from second code)
# ------------------------------
class EnhancedMesoNet(nn.Module):
    """Enhanced MesoNet with improved architecture"""
    def __init__(self, image_size=128):
        super(EnhancedMesoNet, self).__init__()
        self.image_size = image_size

        # First conv block
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(16)
        self.pool1 = nn.MaxPool2d(2, 2)

        # Second conv block
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(32)
        self.pool2 = nn.MaxPool2d(2, 2)

        # Third conv block
        self.conv3 = nn.Conv2d(32, 64, 3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(64)
        self.pool3 = nn.MaxPool2d(2, 2)

        # Fourth conv block
        self.conv4 = nn.Conv2d(64, 128, 3, padding=1, bias=False)
        self.bn4 = nn.BatchNorm2d(128)
        self.pool4 = nn.MaxPool2d(2, 2)

        # Fifth conv block (additional for better feature extraction)
        self.conv5 = nn.Conv2d(128, 256, 3, padding=1, bias=False)
        self.bn5 = nn.BatchNorm2d(256)
        self.pool5 = nn.MaxPool2d(2, 2)

        # Calculate feature dimension
        feature_dim = 256 * (image_size // 32) * (image_size // 32)
        self.feature_size = feature_dim

    def forward(self, x):
        # First block
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        # Second block
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        # Third block
        x = self.pool3(F.relu(self.bn3(self.conv3(x))))
        # Fourth block
        x = self.pool4(F.relu(self.bn4(self.conv4(x))))
        # Fifth block
        x = self.pool5(F.relu(self.bn5(self.conv5(x))))

        return x


class EnhancedMesoNetLSTM(nn.Module):
    """Enhanced MesoNet + LSTM matching the trained architecture"""
    def __init__(self, config=None):
        super(EnhancedMesoNetLSTM, self).__init__()

        if config is None:
            config = TRAINED_CONFIG

        self.config = config

        # Enhanced MesoNet base model
        self.mesonet = EnhancedMesoNet(config['image_size'])
        feature_dim = self.mesonet.feature_size

        # Flatten features
        self.flatten = nn.Flatten()

        # Feature reduction layer
        self.feature_reducer = nn.Sequential(
            nn.Linear(feature_dim, config['lstm_hidden_size']),
            nn.ReLU(),
            nn.BatchNorm1d(config['lstm_hidden_size']),
            nn.Dropout(config['dropout_rate'] * 0.5)
        )

        # LSTM layers
        self.lstm = nn.LSTM(
            input_size=config['lstm_hidden_size'],
            hidden_size=config['lstm_hidden_size'],
            num_layers=config['lstm_layers'],
            batch_first=True,
            bidirectional=True,
            dropout=config['dropout_rate'] if config['lstm_layers'] > 1 else 0
        )

        # Attention mechanism
        self.attention = nn.MultiheadAttention(
            embed_dim=config['lstm_hidden_size'] * 2,
            num_heads=8,
            dropout=config['dropout_rate'],
            batch_first=True
        )

        # Final classification layers
        self.classifier = nn.Sequential(
            nn.Dropout(config['dropout_rate']),
            nn.Linear(config['lstm_hidden_size'] * 2, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(config['dropout_rate'] * 0.5),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Dropout(config['dropout_rate'] * 0.3),
            nn.Linear(64, 1)
        )

        # Store frame features for analysis
        self.frame_activations = []

    def forward(self, x, save_activations=False):
        # x shape: [batch, frames, channels, height, width]
        batch_size, seq_len = x.size(0), x.size(1)

        if save_activations:
            self.frame_activations = []

        # Process each frame through MesoNet
        frame_features = []
        for t in range(seq_len):
            frame = x[:, t, :, :, :]
            features = self.mesonet(frame)

            if save_activations:
                self.frame_activations.append(features.detach())

            features = self.flatten(features)
            features = self.feature_reducer(features)
            frame_features.append(features)

        # Stack features for LSTM
        lstm_input = torch.stack(frame_features, dim=1)

        # LSTM processing
        lstm_out, _ = self.lstm(lstm_input)

        # Apply attention mechanism
        attended_out, _ = self.attention(lstm_out, lstm_out, lstm_out)

        # Global average pooling over sequence dimension
        pooled_out = torch.mean(attended_out, dim=1)

        # Final classification
        output = self.classifier(pooled_out)

        return output

# ------------------------------
# Grad-CAM Implementation (updated for enhanced architecture)
# ------------------------------
class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.activations = None
        self.hooks = []

        # Register hooks
        self._register_hooks()

    def _register_hooks(self):
        def forward_hook(module, input, output):
            self.activations = output.detach()

        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0].detach()

        # Register hooks on the target layer
        self.hooks.append(self.target_layer.register_forward_hook(forward_hook))
        self.hooks.append(self.target_layer.register_full_backward_hook(backward_hook))

    def generate_cam(self, input_tensor, class_idx=None):
        # Set model to training mode for backward pass
        self.model.train()

        # Enable gradients
        input_tensor.requires_grad_(True)

        # Forward pass
        model_output = self.model(input_tensor)

        if class_idx is None:
            class_idx = model_output.argmax(dim=1)

        # Clear gradients
        self.model.zero_grad()

        # Backward pass - use the raw output for binary classification
        model_output[0, 0].backward(retain_graph=True)

        # Generate CAM
        gradients = self.gradients
        activations = self.activations

        if gradients is None or activations is None:
            print("Warning: No gradients or activations captured")
            return np.zeros((4, 4))  # Return empty heatmap for 32x reduction

        # Global average pooling of gradients
        weights = torch.mean(gradients, dim=[2, 3], keepdim=True)

        # Weighted combination of activation maps
        cam = torch.sum(weights * activations, dim=1, keepdim=True)

        # Apply ReLU
        cam = F.relu(cam)

        # Normalize
        if torch.max(cam) > 0:
            cam = cam / torch.max(cam)

        # Set model back to eval mode
        self.model.eval()

        return cam.squeeze().cpu().numpy()

    def remove_hooks(self):
        for hook in self.hooks:
            hook.remove()

# ------------------------------
# Utility Functions (updated for enhanced model)
# ------------------------------
def extract_frames(video_path, num_frames=FRAME_COUNT):
    cap = cv2.VideoCapture(video_path)
    frames = []
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    indices = np.linspace(0, total - 1, num_frames, dtype=int)

    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret:
            continue
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE))
        frames.append(frame)

    cap.release()
    while len(frames) < num_frames:
        frames.append(frames[-1])
    return np.array(frames)

def denormalize_frame(frame_tensor):
    """Denormalize frame for visualization"""
    device = frame_tensor.device
    mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225], device=device).view(3, 1, 1)

    frame = frame_tensor * std + mean
    frame = torch.clamp(frame, 0, 1)
    return frame.permute(1, 2, 0).cpu().numpy()

def overlay_heatmap(image, heatmap, alpha=0.4):
    """Overlay heatmap on image"""
    # Resize heatmap to match image size
    heatmap_resized = cv2.resize(heatmap, (image.shape[1], image.shape[0]))

    # Apply colormap
    heatmap_colored = cm.jet(heatmap_resized)[:, :, :3]

    # Overlay
    overlayed = alpha * heatmap_colored + (1 - alpha) * image
    return overlayed

def load_trained_model(model_path, config=None):
    """Load the trained model with proper configuration"""
    if config is None:
        config = TRAINED_CONFIG

    # Initialize model with the same architecture
    model = EnhancedMesoNetLSTM(config).to(DEVICE)

    # Load the saved state
    try:
        # Try loading as checkpoint format first
        checkpoint = torch.load(model_path, map_location=DEVICE)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            # It's a checkpoint with metadata
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"Loaded model from checkpoint. Accuracy: {checkpoint.get('accuracy', 'N/A')}")
            print(f"Epoch: {checkpoint.get('epoch', 'N/A')}")
        else:
            # It's just the state dict
            model.load_state_dict(checkpoint)
            print("Loaded model state dict")
    except Exception as e:
        print(f"Error loading model: {e}")
        print("Please check the model path and format")
        return None

    model.eval()
    return model

def visualize_enhanced_gradcam(video_path, model_path, num_frames_to_show=10, save_path=None):
    """Enhanced Grad-CAM visualization with the trained model"""

    print(f"Loading enhanced model from: {model_path}")

    # Load the trained model
    model = load_trained_model(model_path, TRAINED_CONFIG)
    if model is None:
        return

    # Transform matching training setup
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    # Extract frames
    print(f"Extracting {FRAME_COUNT} frames from video...")
    frames_np = extract_frames(video_path)
    frames_tensor = torch.stack([transform(frame) for frame in frames_np])
    frames_tensor = frames_tensor.unsqueeze(0).to(DEVICE)

    # Get prediction from full model
    print("Getting model prediction...")
    with torch.no_grad():
        output = model(frames_tensor, save_activations=True)
        prob = torch.sigmoid(output).item()
        label = "FAKE" if prob > 0.5 else "REAL"

    print(f"Prediction: {label} (Confidence: {prob:.4f})")

    # Create a CNN-only model for Grad-CAM (using the enhanced architecture)
    class CNNOnlyEnhanced(nn.Module):
        def __init__(self, base_model):
            super().__init__()
            self.mesonet = base_model.mesonet
            self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
            self.classifier = nn.Sequential(
                nn.Linear(256, 128),  # Updated for enhanced architecture (256 features)
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, 1)
            )

        def forward(self, x):
            features = self.mesonet(x)
            pooled = self.global_pool(features)
            pooled = pooled.view(pooled.size(0), -1)
            return self.classifier(pooled)

    # Create CNN model for Grad-CAM
    cnn_model = CNNOnlyEnhanced(model).to(DEVICE)
    cnn_model.eval()

    # Select frames to visualize
    frame_indices = np.linspace(0, FRAME_COUNT-1, num_frames_to_show, dtype=int)

    # Create subplots with better layout for frames
    fig, axes = plt.subplots(3, len(frame_indices), figsize=(25, 12))
    if len(frame_indices) == 1:
        axes = axes.reshape(3, 1)

    # List to store all heatmaps for aggregation
    all_heatmaps = []

    for i, frame_idx in enumerate(frame_indices):
        # Get single frame
        single_frame = frames_tensor[:, frame_idx, :, :, :]

        # Generate Grad-CAM using the enhanced architecture
        gradcam = GradCAM(cnn_model, cnn_model.mesonet.conv5)

        try:
            heatmap = gradcam.generate_cam(single_frame)
            all_heatmaps.append(heatmap)  # Save for aggregation

            # Original frame for visualization
            original_frame = denormalize_frame(frames_tensor[0, frame_idx])

            # Plot original frame
            axes[0, i].imshow(original_frame)
            axes[0, i].set_title(f'Frame {frame_idx+1}')
            axes[0, i].axis('off')

            # Plot heatmap
            axes[1, i].imshow(heatmap, cmap='jet')
            axes[1, i].set_title(f'Attention Map')
            axes[1, i].axis('off')

            # Plot overlay
            overlayed = overlay_heatmap(original_frame, heatmap)
            axes[2, i].imshow(overlayed)
            axes[2, i].set_title(f'Overlay')
            axes[2, i].axis('off')

        except Exception as e:
            print(f"Warning: Could not generate Grad-CAM for frame {frame_idx}: {e}")
            original_frame = denormalize_frame(frames_tensor[0, frame_idx])
            all_heatmaps.append(np.zeros((heatmap.shape if 'heatmap' in locals() else (4,4))))  # fallback zero heatmap
            for row in range(3):
                axes[row, i].imshow(original_frame)
                axes[row, i].set_title(f'Frame {frame_idx+1}')
                axes[row, i].axis('off')

        gradcam.remove_hooks()

    # AGGREGATED HEATMAP OVER REPRESENTATIVE FRAME
    if len(all_heatmaps) > 0:
        # Sum all heatmaps
        aggregated_heatmap = np.sum(np.stack(all_heatmaps), axis=0)

        # Normalize aggregated heatmap to [0,1]
        aggregated_heatmap -= aggregated_heatmap.min()
        if aggregated_heatmap.max() > 0:
            aggregated_heatmap /= aggregated_heatmap.max()

        # Choose representative frame index (middle frame)
        rep_frame_idx = frame_indices[len(frame_indices)//2]
        rep_frame = denormalize_frame(frames_tensor[0, rep_frame_idx])

        # Overlay aggregated heatmap on representative frame
        aggregated_overlay = overlay_heatmap(rep_frame, aggregated_heatmap)

        # Add new figure below or as a separate figure
        plt.figure(figsize=(6, 6))
        plt.suptitle(f'Aggregated Grad-CAM Overlay on Frame {rep_frame_idx+1}', fontsize=14, fontweight='bold')

        plt.subplot(1, 3, 1)
        plt.imshow(rep_frame)
        plt.title(f'Representative Frame {rep_frame_idx+1}')
        plt.axis('off')

        plt.subplot(1, 3, 2)
        plt.imshow(aggregated_heatmap, cmap='jet')
        plt.title('Aggregated Heatmap')
        plt.axis('off')

        plt.subplot(1, 3, 3)
        plt.imshow(aggregated_overlay)
        plt.title('Overlay (Aggregated)')
        plt.axis('off')

        if save_path:
            # Save aggregated visualization as separate file
            base_save_path = Path(save_path)
            agg_save_path = base_save_path.with_name(base_save_path.stem + '_aggregated.png')
            plt.savefig(agg_save_path, dpi=300, bbox_inches='tight')
            print(f"Aggregated visualization saved to {agg_save_path}")

        plt.show()

    # Original title and plot layout for frame-wise visualization
    plt.suptitle(f'Enhanced Grad-CAM Analysis - {num_frames_to_show} Frame Demonstration\nModel: {TRAINED_CONFIG["name"]} | '
                f'Prediction: {label} (Conf: {prob:.3f}) | '
                f'Architecture: Enhanced MesoNet+LSTM',
                fontsize=16, fontweight='bold')
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Visualization saved to {save_path}")

    plt.show()

def analyze_video_enhanced(video_path, model_path, save_path=None):
    """Main function to analyze a video with the enhanced trained model"""
    print(f"Analyzing video: {video_path}")
    print(f"Using enhanced trained model: {model_path}")
    print(f"Configuration: {TRAINED_CONFIG}")

    if save_path is None:
        save_path = "enhanced_gradcam_10frames_analysis.png"

    visualize_enhanced_gradcam(video_path, model_path, save_path=save_path)

# Example usage functions for the enhanced model
def load_dataset_metadata(base_path, csv_filename="global_metadata_cleaned.csv"):
    """Load the dataset metadata"""
    csv_path = os.path.join(base_path, csv_filename)
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"Metadata file not found: {csv_path}")

    df = pd.read_csv(csv_path)
    print(f"Loaded metadata with {len(df)} entries")
    print(f"Datasets: {df['dataset'].unique()}")
    print(f"Labels: {df['label'].value_counts()}")
    print(f"Subsets: {df['subset'].value_counts()}")

    return df

def analyze_random_videos_enhanced(base_path, model_path, num_videos=3, **filter_kwargs):
    """Analyze multiple random videos with the enhanced model"""

    # Load metadata
    df = load_dataset_metadata(base_path)

    # Apply filters
    filtered_df = df.copy()

    if 'dataset_filter' in filter_kwargs and filter_kwargs['dataset_filter']:
        filtered_df = filtered_df[filtered_df['dataset'].isin(filter_kwargs['dataset_filter'])]
        print(f"Filtered by dataset {filter_kwargs['dataset_filter']}: {len(filtered_df)} videos")

    if 'label_filter' in filter_kwargs and filter_kwargs['label_filter']:
        filtered_df = filtered_df[filtered_df['label'].isin(filter_kwargs['label_filter'])]
        print(f"Filtered by label {filter_kwargs['label_filter']}: {len(filtered_df)} videos")

    if 'subset_filter' in filter_kwargs:
        subset_filter = filter_kwargs.get('subset_filter', 'test')
        filtered_df = filtered_df[filtered_df['subset'] == subset_filter]
        print(f"Filtered by subset {subset_filter}: {len(filtered_df)} videos")

    if len(filtered_df) == 0:
        raise ValueError("No videos found matching the specified filters")

    # Sample random videos
    num_videos = min(num_videos, len(filtered_df))
    sampled_df = filtered_df.sample(n=num_videos, random_state=None)

    print(f"\nAnalyzing {num_videos} random videos with enhanced model:")
    print("Showing 10 frames per video for detailed temporal analysis:")
    for idx, row in sampled_df.iterrows():
        print(f"  {row['file_path']} - {row['label']} ({row['dataset']})")

    # Analyze each video
    for idx, (_, row) in enumerate(sampled_df.iterrows()):
        video_path = os.path.join(base_path, row['file_path'])
        true_label = row['label']
        dataset_name = row['dataset']

        print(f"\n--- Enhanced Analysis {idx+1}/{len(sampled_df)} ---")
        print(f"File: {row['file_path']}")
        print(f"True Label: {true_label}")
        print(f"Dataset: {dataset_name}")

        if not os.path.exists(video_path):
            print(f"❌ Video file not found: {video_path}")
            continue

        try:
            save_path = f"enhanced_gradcam_10frames_{idx+1}_{dataset_name}_{true_label}.png"
            analyze_video_enhanced(video_path, model_path, save_path)
        except Exception as e:
            print(f"❌ Error analyzing {video_path}: {e}")
            continue

def analyze_batch_videos_enhanced(base_path, model_path, analysis_configs):
    """
    Analyze multiple batches of videos with different configurations

    Args:
        base_path: Dataset base path
        model_path: Trained model path
        analysis_configs: List of analysis configurations

    Example:
        configs = [
            {'name': 'FAKE_Videos', 'num_videos': 5, 'label_filter': ['FAKE']},
            {'name': 'REAL_Videos', 'num_videos': 5, 'label_filter': ['REAL']},
        ]
    """
    print("🎯 BATCH ANALYSIS MODE - MULTIPLE VIDEO SETS")
    print("="*60)

    total_videos = sum(config['num_videos'] for config in analysis_configs)
    print(f"Total videos to analyze: {total_videos}")
    print(f"Batch configurations: {len(analysis_configs)}")

    for i, config in enumerate(analysis_configs, 1):
        print(f"\n--- BATCH {i}/{len(analysis_configs)}: {config['name']} ---")

        try:
            analyze_random_videos_enhanced(
                base_path,
                model_path,
                **config
            )
            print(f"✅ Completed batch: {config['name']}")

        except Exception as e:
            print(f"❌ Error in batch {config['name']}: {e}")
            continue

    print(f"\n🎉 BATCH ANALYSIS COMPLETE!")
    print(f"Generated {total_videos} comprehensive video analyses")
    print(f"Each with 10 frames = {total_videos * 10} total frame analyses")

# Additional convenience functions for different analysis scenarios
def analyze_diverse_dataset_demo(base_path, model_path):
    """Demonstrate analysis across diverse video types"""

    analysis_configs = [
        {
            'name': 'DFDC_FAKE_Videos',
            'num_videos': 3,
            'dataset_filter': ['DFDC'],
            'label_filter': ['FAKE'],
            'subset_filter': 'test'
        },
        {
            'name': 'DFDC_REAL_Videos',
            'num_videos': 3,
            'dataset_filter': ['DFDC'],
            'label_filter': ['REAL'],
            'subset_filter': 'test'
        },
        {
            'name': 'Mixed_Dataset_Videos',
            'num_videos': 4,
            'dataset_filter': ['DFDC'],
            'subset_filter': 'test'
        }
    ]

    print("🚀 DIVERSE DATASET DEMONSTRATION")
    print("Analyzing 10 videos total: 3 FAKE + 3 REAL + 4 Mixed")

    analyze_batch_videos_enhanced(base_path, model_path, analysis_configs)

# Example usage
if __name__ == "__main__":
    # Update these paths to match your setup
    base_path = "/content/drive/MyDrive/Dataset-3"

    # Use the trained model from the results
    # Based on your training results, the model should be saved as:
    model_path = "/content/drive/MyDrive/Dataset-3/dfdc_training_run_20250527_143110/models/best_model_base_config.pth"

    print("Enhanced Grad-CAM Analysis with Trained Model")
    print("=" * 50)
    print(f"Using configuration: {TRAINED_CONFIG}")
    print(f"Frame count: {FRAME_COUNT}")
    print(f"Image size: {IMAGE_SIZE}")
    print(f"LSTM hidden size: {TRAINED_CONFIG['lstm_hidden_size']}")
    print(f"Model path: {model_path}")

    # Example 1: Analyze 10 different random DFDC videos with 10 frames each
    print("\nAnalyzing 10 different random DFDC videos with 10-frame demonstration...")
    try:
        analyze_random_videos_enhanced(
            base_path,
            model_path,
            num_videos=10,
            dataset_filter=['DFDC'],
            subset_filter='test'
        )
    except Exception as e:
        print(f"Error: {e}")
        print("\nPlease verify:")
        print("1. The model path exists and contains the trained weights")
        print("2. The base_path contains the dataset and metadata CSV")
        print("3. The dataset structure matches the expected format")
        print("\n--- Alternative: Multiple analysis examples ---")
        print("# For analyzing 5 FAKE videos specifically:")
        print("# analyze_random_videos_enhanced(base_path, model_path, num_videos=5, dataset_filter=['DFDC'], label_filter=['FAKE'])")
        print("# ")
        print("# For analyzing 5 REAL videos specifically:")
        print("# analyze_random_videos_enhanced(base_path, model_path, num_videos=5, dataset_filter=['DFDC'], label_filter=['REAL'])")
        print("# ")
        print("# For analyzing mixed FF and DFDC datasets:")
        print("# analyze_random_videos_enhanced(base_path, model_path, num_videos=10, dataset_filter=['DFDC', 'FF'])")
        print("# ")
        print("# For single video analysis:")
        print("# video_path = '/path/to/your/video.mp4'")
        print("# analyze_video_enhanced(video_path, model_path)")

    print("\n🎯 OPTION 2: Diverse Dataset Demonstration (Alternative)")
    print("-" * 50)
    print("# Uncomment to run diverse analysis:")
    print("# analyze_diverse_dataset_demo(base_path, model_path)")

    print("\n🎯 OPTION 3: Custom Batch Analysis (Advanced)")
    print("-" * 50)
    print("# Example custom configuration:")
    print("# custom_configs = [")
    print("#     {'name': 'High_Confidence_FAKE', 'num_videos': 3, 'label_filter': ['FAKE']},")
    print("#     {'name': 'High_Confidence_REAL', 'num_videos': 3, 'label_filter': ['REAL']},")
    print("#     {'name': 'Mixed_Analysis', 'num_videos': 4}")
    print("# ]")
    print("# analyze_batch_videos_enhanced(base_path, model_path, custom_configs)")